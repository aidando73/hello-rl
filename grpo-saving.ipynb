{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.3.14: Fast Llama patching. Transformers: 4.49.0. vLLM: 0.7.3.\n",
      "   \\\\   /|    NVIDIA H100 80GB HBM3. Num GPUs = 1. Max memory: 79.205 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 9.0. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.3.14 patched 32 layers with 32 QKV layers, 32 O layers and 32 MLP layers.\n",
      "Unsloth: You are pushing to hub, but you passed your HF username = aidando73.\n",
      "We shall truncate aidando73/llama-3.1-8b-grpo-4bit-merged to llama-3.1-8b-grpo-4bit-merged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Merging 4bit and LoRA weights to 16bit...\n",
      "Unsloth: Will use up to 1076.53 out of 1511.36 RAM for saving.\n",
      "Unsloth: Saving model... This might take 5 minutes ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:00<00:00, 79.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Saving tokenizer... Done.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import os\n",
    "\n",
    "SAVED_MODEL = \"grpo_saved_lora\"\n",
    "MODEL_NAME = \"aidando73/llama-3.1-8b-grpo-4bit-merged\"\n",
    "\n",
    "max_seq_length = 1024\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = SAVED_MODEL, # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = True\n",
    ")\n",
    "\n",
    "model.push_to_hub_merged(MODEL_NAME, tokenizer, save_method = \"merged_16bit\", token = os.getenv(\"HUGGINGFACE_TOKEN\"))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
