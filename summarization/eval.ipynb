{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/hello-rl/summarization/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (6553, 2)\n",
      "                                              prompt  \\\n",
      "0  SUBREDDIT: r/relationships\\n\\nTITLE: Me [19 F]...   \n",
      "1  SUBREDDIT: r/Parenting\\n\\nTITLE: My 11 year ol...   \n",
      "2  SUBREDDIT: r/relationships\\n\\nTITLE: The girl ...   \n",
      "3  SUBREDDIT: r/tifu\\n\\nTITLE: TIFU by accidently...   \n",
      "4  SUBREDDIT: r/relationships\\n\\nTITLE: I [32 M] ...   \n",
      "\n",
      "                                          completion  \n",
      "0   I really like this guy, but after having sex ...  \n",
      "1   Sons good friend died and his funeral is toda...  \n",
      "2   Girl I'm seeing didn't respond to my texts wh...  \n",
      "3   Tried to stop an old lady falling, kicked her...  \n",
      "4   Wife Cheats on me but I stuck around for kids...  \n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "dataset = datasets.load_dataset(\"trl-lib/tldr\")\n",
    "\n",
    "\n",
    "# Convert the dataset to a pandas DataFrame\n",
    "df = pd.DataFrame(dataset['test'])\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved to: eval/inference/Qwen_Qwen2-0.5B-Instruct_baseline_2025-03-16_07-16.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Processing batches: 100%|██████████| 820/820 [14:48<00:00,  1.08s/it]\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen2-0.5B-Instruct\"\n",
    "KEY_WORD = \"baseline\"\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Batch size for processing\n",
    "SYSTEM_PROMPT = \"Summarize the following text within 20 characters: {text}\"\n",
    "\n",
    "\n",
    "# Create a file to store the inference results\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create the inference directory if it doesn't exist\n",
    "inference_dir = \"eval/inference/\"\n",
    "os.makedirs(inference_dir, exist_ok=True)\n",
    "\n",
    "# Generate a filename with the model name and current date\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "filename = f\"{inference_dir}{MODEL_NAME.replace('/', '_')}_{KEY_WORD}_{current_date}.jsonl\"\n",
    "\n",
    "print(f\"Results will be saved to: {filename}\")\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize the pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, padding_side=\"left\")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    MODEL_NAME,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Function to generate summary for a single text\n",
    "def generate_summary(text):\n",
    "    prompt = SYSTEM_PROMPT.format(text=text)\n",
    "    response = pipe(prompt, max_new_tokens=418, do_sample=True)\n",
    "    return response[0]['generated_text']\n",
    "\n",
    "# Process the dataset in batches\n",
    "for i in tqdm(range(0, len(df), BATCH_SIZE), desc=\"Processing batches\"):\n",
    "    batch_prompts = df['prompt'][i:i+BATCH_SIZE].tolist()\n",
    "    batch = [[{\"role\": \"system\", \"content\": SYSTEM_PROMPT.format(text=text)}] for text in batch_prompts]\n",
    "    \n",
    "    # Generate summaries for the batch\n",
    "    outputs = pipe(\n",
    "        batch,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        batch_size=BATCH_SIZE,\n",
    "    )\n",
    "    \n",
    "    # Extract the generated summaries\n",
    "    results = [\n",
    "        {\n",
    "            \"prompt\": prompt,\n",
    "            \"summary\": output[0]['generated_text'][-1]['content']\n",
    "        } for prompt, output in zip(batch, outputs)\n",
    "    ]\n",
    "\n",
    "    # Write the summaries to the file\n",
    "    with open(filename, \"a\") as f:\n",
    "        for result in results:\n",
    "            f.write(json.dumps(result) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
