{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/hello-rl/summarization/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (6553, 2)\n",
      "                                              prompt  \\\n",
      "0  SUBREDDIT: r/relationships\\n\\nTITLE: Me [19 F]...   \n",
      "1  SUBREDDIT: r/Parenting\\n\\nTITLE: My 11 year ol...   \n",
      "2  SUBREDDIT: r/relationships\\n\\nTITLE: The girl ...   \n",
      "3  SUBREDDIT: r/tifu\\n\\nTITLE: TIFU by accidently...   \n",
      "4  SUBREDDIT: r/relationships\\n\\nTITLE: I [32 M] ...   \n",
      "\n",
      "                                          completion  \n",
      "0   I really like this guy, but after having sex ...  \n",
      "1   Sons good friend died and his funeral is toda...  \n",
      "2   Girl I'm seeing didn't respond to my texts wh...  \n",
      "3   Tried to stop an old lady falling, kicked her...  \n",
      "4   Wife Cheats on me but I stuck around for kids...  \n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "dataset = datasets.load_dataset(\"trl-lib/tldr\")\n",
    "\n",
    "\n",
    "# Convert the dataset to a pandas DataFrame\n",
    "df = pd.DataFrame(dataset['test'])\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results will be saved to: inference/Qwen_Qwen2-0.5B-Instruct_2025-03-16_07-08.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Processing batches:   0%|          | 0/820 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The following `model_kwargs` are not used by the model: ['padding_side'] (note: typos in the generate arguments will also show up in this list)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/workspace/hello-rl/summarization/eval.ipynb Cell 2\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B38.80.152.248/workspace/hello-rl/summarization/eval.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=42'>43</a>\u001b[0m batch \u001b[39m=\u001b[39m [[{\u001b[39m\"\u001b[39m\u001b[39mrole\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39msystem\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m\"\u001b[39m: SYSTEM_PROMPT\u001b[39m.\u001b[39mformat(text\u001b[39m=\u001b[39mtext)}] \u001b[39mfor\u001b[39;00m text \u001b[39min\u001b[39;00m batch_prompts]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B38.80.152.248/workspace/hello-rl/summarization/eval.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m \u001b[39m# Generate summaries for the batch\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B38.80.152.248/workspace/hello-rl/summarization/eval.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m outputs \u001b[39m=\u001b[39m pipe(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B38.80.152.248/workspace/hello-rl/summarization/eval.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m     batch,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B38.80.152.248/workspace/hello-rl/summarization/eval.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m     max_new_tokens\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B38.80.152.248/workspace/hello-rl/summarization/eval.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m     do_sample\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B38.80.152.248/workspace/hello-rl/summarization/eval.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m     batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B38.80.152.248/workspace/hello-rl/summarization/eval.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m     padding_side\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mleft\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B38.80.152.248/workspace/hello-rl/summarization/eval.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B38.80.152.248/workspace/hello-rl/summarization/eval.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39m# Extract the generated summaries\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B38.80.152.248/workspace/hello-rl/summarization/eval.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=54'>55</a>\u001b[0m results \u001b[39m=\u001b[39m [\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B38.80.152.248/workspace/hello-rl/summarization/eval.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=55'>56</a>\u001b[0m     {\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B38.80.152.248/workspace/hello-rl/summarization/eval.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=56'>57</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mprompt\u001b[39m\u001b[39m\"\u001b[39m: prompt,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B38.80.152.248/workspace/hello-rl/summarization/eval.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=57'>58</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msummary\u001b[39m\u001b[39m\"\u001b[39m: output[\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mgenerated_text\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mcontent\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B38.80.152.248/workspace/hello-rl/summarization/eval.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=58'>59</a>\u001b[0m     } \u001b[39mfor\u001b[39;00m prompt, output \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(batch, outputs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B38.80.152.248/workspace/hello-rl/summarization/eval.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=59'>60</a>\u001b[0m ]\n",
      "File \u001b[0;32m/workspace/hello-rl/summarization/env/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:286\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(chats, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    285\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 286\u001b[0m                 \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(\u001b[39mlist\u001b[39;49m(chats), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    287\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__call__\u001b[39m(text_inputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/workspace/hello-rl/summarization/env/lib/python3.10/site-packages/transformers/pipelines/base.py:1349\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1345\u001b[0m \u001b[39mif\u001b[39;00m can_use_iterator:\n\u001b[1;32m   1346\u001b[0m     final_iterator \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_iterator(\n\u001b[1;32m   1347\u001b[0m         inputs, num_workers, batch_size, preprocess_params, forward_params, postprocess_params\n\u001b[1;32m   1348\u001b[0m     )\n\u001b[0;32m-> 1349\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39;49m(final_iterator)\n\u001b[1;32m   1350\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs\n\u001b[1;32m   1351\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/workspace/hello-rl/summarization/env/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader_batch_item()\n\u001b[1;32m    123\u001b[0m \u001b[39m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 124\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterator)\n\u001b[1;32m    125\u001b[0m processed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfer(item, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams)\n\u001b[1;32m    126\u001b[0m \u001b[39m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/hello-rl/summarization/env/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:125\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[39m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    124\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterator)\n\u001b[0;32m--> 125\u001b[0m processed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfer(item, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams)\n\u001b[1;32m    126\u001b[0m \u001b[39m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader_batch_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    128\u001b[0m     \u001b[39m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/hello-rl/summarization/env/lib/python3.10/site-packages/transformers/pipelines/base.py:1275\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1273\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1274\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m-> 1275\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m   1276\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m   1277\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/workspace/hello-rl/summarization/env/lib/python3.10/site-packages/transformers/pipelines/text_generation.py:385\u001b[0m, in \u001b[0;36mTextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mgeneration_config\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m generate_kwargs:\n\u001b[1;32m    383\u001b[0m     generate_kwargs[\u001b[39m\"\u001b[39m\u001b[39mgeneration_config\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgeneration_config\n\u001b[0;32m--> 385\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mgenerate(input_ids\u001b[39m=\u001b[39;49minput_ids, attention_mask\u001b[39m=\u001b[39;49mattention_mask, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgenerate_kwargs)\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output, ModelOutput):\n\u001b[1;32m    388\u001b[0m     generated_sequence \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39msequences\n",
      "File \u001b[0;32m/workspace/hello-rl/summarization/env/lib/python3.10/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/workspace/hello-rl/summarization/env/lib/python3.10/site-packages/transformers/generation/utils.py:1983\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1980\u001b[0m assistant_tokenizer \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39massistant_tokenizer\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)  \u001b[39m# only used for assisted generation\u001b[39;00m\n\u001b[1;32m   1982\u001b[0m generation_config, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_prepare_generation_config(generation_config, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m-> 1983\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_model_kwargs(model_kwargs\u001b[39m.\u001b[39;49mcopy())\n\u001b[1;32m   1984\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_assistant(assistant_model, tokenizer, assistant_tokenizer)\n\u001b[1;32m   1986\u001b[0m \u001b[39m# 2. Set generation parameters if not already defined\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/hello-rl/summarization/env/lib/python3.10/site-packages/transformers/generation/utils.py:1393\u001b[0m, in \u001b[0;36mGenerationMixin._validate_model_kwargs\u001b[0;34m(self, model_kwargs)\u001b[0m\n\u001b[1;32m   1390\u001b[0m         unused_model_args\u001b[39m.\u001b[39mappend(key)\n\u001b[1;32m   1392\u001b[0m \u001b[39mif\u001b[39;00m unused_model_args:\n\u001b[0;32m-> 1393\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1394\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThe following `model_kwargs` are not used by the model: \u001b[39m\u001b[39m{\u001b[39;00munused_model_args\u001b[39m}\u001b[39;00m\u001b[39m (note: typos in the\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1395\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m generate arguments will also show up in this list)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1396\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: The following `model_kwargs` are not used by the model: ['padding_side'] (note: typos in the generate arguments will also show up in this list)"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"Qwen/Qwen2-0.5B-Instruct\"\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Batch size for processing\n",
    "SYSTEM_PROMPT = \"Summarize the following text within 20 characters: {text}\"\n",
    "\n",
    "\n",
    "# Create a file to store the inference results\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Create the inference directory if it doesn't exist\n",
    "inference_dir = \"eval/inference/\"\n",
    "os.makedirs(inference_dir, exist_ok=True)\n",
    "\n",
    "# Generate a filename with the model name and current date\n",
    "current_date = datetime.now().strftime(\"%Y-%m-%d_%H-%M\")\n",
    "filename = f\"{inference_dir}/{MODEL_NAME.replace('/', '_')}_{current_date}.jsonl\"\n",
    "\n",
    "print(f\"Results will be saved to: {filename}\")\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize the pipeline\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Function to generate summary for a single text\n",
    "def generate_summary(text):\n",
    "    prompt = SYSTEM_PROMPT.format(text=text)\n",
    "    response = pipe(prompt, max_new_tokens=418, do_sample=True)\n",
    "    return response[0]['generated_text']\n",
    "\n",
    "# Process the dataset in batches\n",
    "for i in tqdm(range(0, len(df), BATCH_SIZE), desc=\"Processing batches\"):\n",
    "    batch_prompts = df['prompt'][i:i+BATCH_SIZE].tolist()\n",
    "    batch = [[{\"role\": \"system\", \"content\": SYSTEM_PROMPT.format(text=text)}] for text in batch_prompts]\n",
    "    \n",
    "    # Generate summaries for the batch\n",
    "    outputs = pipe(\n",
    "        batch,\n",
    "        max_new_tokens=50,\n",
    "        do_sample=True,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        padding_side=\"left\"\n",
    "    )\n",
    "    \n",
    "    # Extract the generated summaries\n",
    "    results = [\n",
    "        {\n",
    "            \"prompt\": prompt,\n",
    "            \"summary\": output[0]['generated_text'][-1]['content']\n",
    "        } for prompt, output in zip(batch, outputs)\n",
    "    ]\n",
    "\n",
    "    # Write the summaries to the file\n",
    "    with open(filename, \"a\") as f:\n",
    "        for result in results:\n",
    "            f.write(json.dumps(result) + \"\\n\")\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Processed {min(i+BATCH_SIZE, len(df))}/{len(df)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
